import logging
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from datetime import datetime, timedelta
from pymongo import MongoClient, errors
from confluent_kafka import Producer, KafkaException

# Default arguments for the DAG
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2023, 7, 10),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

# Parameterized configurations
KAFKA_TOPIC = '<your_kafka_topic>'
KAFKA_CONF = {
    'bootstrap.servers': '<your_kafka_broker>'
}

MONGO_DB1_URI = '<mongodb1_uri>'
MONGO_DB1_NAME = 'abcd_steel'
MONGO_DB1_COLLECTION = 'steel_events'

MONGO_DB2_URI = '<mongodb2_uri>'
MONGO_DB2_NAME = 'abcd_rust'
MONGO_DB2_COLLECTION = 'rust_events'

# Function to get today's date in yyyymmdd format
def get_today_date():
    return datetime.now().strftime('%Y%m%d')

# DAG definition
dag = DAG(
    'mongodb_to_kafka',
    default_args=default_args,
    description='A simple DAG to read from MongoDB and send to Kafka',
    schedule_interval='@daily',  # Run at midnight every day
)

def connect_to_mongo(uri, db_name, collection_name):
    try:
        client = MongoClient(uri)
        db = client[db_name]
        collection = db[collection_name]
        logging.info("Connected to MongoDB collection: {} in database: {}".format(collection_name, db_name))
        return collection
    except errors.PyMongoError as e:
        logging.error("Failed to connect to MongoDB: {}".format(e))
        raise

def fetch_idBBUniques(execution_date, **context):
    try:
        today_date = execution_date.strftime('%Y%m%d')
        collection = connect_to_mongo(MONGO_DB1_URI, MONGO_DB1_NAME, MONGO_DB1_COLLECTION)
        query = {
            'eventDetails.vendorCode': 'BBCANS',
            'eventDetails.eventType': 'STOCK_SPLT',
            'eventDetails.adjustmentDate': today_date
        }
        documents = collection.find(query)
        idBBUnique_list = [doc['idBBUnique'] for doc in documents]
        logging.info("Fetched idBBUnique list: {}".format(idBBUnique_list))
        return idBBUnique_list
    except errors.PyMongoError as e:
        logging.error("Error fetching idBBUniques: {}".format(e))
        raise

def fetch_latest_data_from_raw(**context):
    try:
        idBBUnique_list = context['task_instance'].xcom_pull(task_ids='fetch_idBBUniques')
        if not idBBUnique_list:
            logging.warning("No idBBUnique found.")
            return

        collection = connect_to_mongo(MONGO_DB2_URI, MONGO_DB2_NAME, MONGO_DB2_COLLECTION)
        data = []
        for idBBUnique in idBBUnique_list:
            pipeline = [
                {'$match': {'message.ID_BB_UNIQUE': idBBUnique, 'src': 'Bloomberg:CANS'}},
                {'$sort': {'message.ID_BB_UNIQUE': 1, 'message.EFF_DT': 1, 'createdAt': -1}},
                {
                    '$group': {
                        '_id': {'ID_BB_UNIQUE': '$message.ID_BB_UNIQUE', 'EFF_DT': '$message.EFF_DT'},
                        'latest_record': {'$first': '$$ROOT'}
                    }
                },
                {'$replaceRoot': {'newRoot': '$latest_record'}}
            ]
            raw_data = list(collection.aggregate(pipeline))
            if raw_data:
                data.extend(raw_data)
        logging.info("Fetched latest data from raw: {}".format(data))
        context['task_instance'].xcom_push(key='raw_data', value=data)
    except errors.PyMongoError as e:
        logging.error("Error fetching latest data from raw: {}".format(e))
        raise

def send_to_kafka(**context):
    try:
        data = context['task_instance'].xcom_pull(key='raw_data', task_ids='fetch_latest_data_from_raw')
        if not data:
            logging.warning("No data to send to Kafka.")
            return

        producer = Producer(KAFKA_CONF)
        for item in data:
            producer.produce(KAFKA_TOPIC, value=str(item))
            logging.info("Sent data to Kafka: {}".format(item))
        producer.flush()
    except KafkaException as e:
        logging.error("Error sending data to Kafka: {}".format(e))
        raise

fetch_idBBUniques_task = PythonOperator(
    task_id='fetch_idBBUniques',
    python_callable=fetch_idBBUniques,
    provide_context=True,
    dag=dag,
)

fetch_latest_data_from_raw_task = PythonOperator(
    task_id='fetch_latest_data_from_raw',
    python_callable=fetch_latest_data_from_raw,
    provide_context=True,
    dag=dag,
)

send_to_kafka_task = PythonOperator(
    task_id='send_to_kafka',
    python_callable=send_to_kafka,
    provide_context=True,
    dag=dag,
)

fetch_idBBUniques_task >> fetch_latest_data_from_raw_task >> send_to_kafka_task
